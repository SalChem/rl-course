Multi-arm bandits
----

__Introductory__:

- Multi-Armed Bandits Intro (15:41) [Video](https://www.youtube.com/watch?v=qAvY2tkMHHA)

__Required__:

- Review course notes from MSDS's Design of Experiments Course
- We'll be implementing the following algorithms in class:
    + Random 
    + Epsilon-greedy
    + Soft-max
    + Bayesian bandits (aka, Thompson sampling)

__Optional__:

For most of the course, we'll be using Stanford's lectures. That instructor chooses to put multi-arm bandits at the end and I choose to put it at the beginning. Given that you are already have seen the material in Design of Experiments, it should not be too hard to follow along:

- Stanford CS234: Reinforcement Learning | Lecture 11 - Fast Reinforcement Learning
    + [Video](https://www.youtube.com/watch?v=RN8qpSs8ozY&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=11)
    + [Slides & Draft lecture notes](http://web.stanford.edu/class/cs234/schedule.html)
- Stanford CS234: Reinforcement Learning | Lecture 12 - Fast Reinforcement Learning II
    + [Video](https://www.youtube.com/watch?v=jJ7JbQBTChM&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=12)
    + [Slides & Draft lecture notes](http://web.stanford.edu/class/cs234/schedule.html)
- SB (Sutton and Barton) Chp 2
- [20 lines of code that will beat A/B testing every time](http://stevehanov.ca/blog/index.php?id=132)
- [Efficient experimentation and the multi-armed bandit](http://iosband.github.io/2015/07/19/Efficient-experimentation-and-multi-armed-bandits.html)
- [Multi-Armed Bandits](http://blog.thedataincubator.com/2016/07/multi-armed-bandits-2/)
- [Beer bandit](http://blog.yhat.com/posts/the-beer-bandit.html)
- [Multi-Armed Bandits](https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits) 

__Challenge__:

- Reinforcement Learning and Multi-Armed Bandits (40 minutes)
    + [Video](https://www.youtube.com/watch?v=aAdD2XRC044)
    + [Slides](https://github.com/brianfarris/RLtalk/blob/master/RLtalk.ipynb)
- [Using multi-arm bandits for growth products](http://www.unofficialgoogledatascience.com/2019/04/misadventures-in-experiments-for-growth.html)
- [A collection of Bandit algorithms](http://banditalgs.com/)
- "Taking the Human Out of the Loop: A Review of Bayesian Optimization" in readings folder
- [The Contextual Bandits Problem: A New, Fast, and Simple Algorithm from Microsoft Research](https://www.youtube.com/watch?v=gzxRDw3lXv8)
- [Bandit Algorithms for Website Optimization](http://shop.oreilly.com/product/0636920027393.do)
- [Sigopt blog](https://blog.sigopt.com/)