{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Multi-arm Bandits Algorithms</h2></center>\n",
    "\n",
    "\n",
    "<center><img src=\"http://i1.wp.com/banditalgs.com/wp-content/uploads/2016/09/cropped-bandit-algorithm-full.png?fit=512%2C512\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "Explain and code the following multi-arm bandits algorithms:\n",
    "\n",
    "1. Random\n",
    "1. Epsilon-Greedy Algorithm  \n",
    "2. Softmax  \n",
    "4. Bayesian Bandit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Random Algorithm</h2></center>\n",
    "\n",
    "1) Pure exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Good for checking the entire system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Reasonable baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Let's Code a Bandit</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "palette = \"Dark2\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import common packages\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Seed the random number generators so you get the same results every time.\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Bandit is the class that setups and runs our multi-arm bandit\n",
    "from bandit import Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def random_choice(self): \n",
    "    \"Pick a bandit uniformly at random.\"\n",
    "    pass\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def random_choice(self): \n",
    "    \"Pick a bandit uniformly at random.\"\n",
    "    return np.random.randint(low=0, high=self.n_bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def random_choice(self): \n",
    "    \"Pick a bandit uniformly at random.\"\n",
    "    return np.random.randint(low=0, high=self.n_bandits) # Return the index of the winning bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Why use `np.random.randint` instead of `random.choice`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`random.choice` is much slower because it selects among any options.\n",
    "\n",
    "`np.random.randint` is optimitize to just select integers (the index of the arm in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.6, 0.6, 0.6, 0.6],\n",
    "           choice_function=random_choice)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "b.max_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.60   0.60   0.60   0.60\n",
      "Number of wins:      165    155    132    147\n",
      "Number of trials:    263    258    243    236\n",
      "Conversion rates:   0.63   0.60   0.54   0.62\n",
      "\n",
      "A total of 599 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "from bandit import print_results\n",
    "\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/eg.jpg\" width=\"55%\"/></center>\n",
    "\n",
    "Some percent of the time we explore - randomly choose one of the options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The rest of the time we exploit - choose the option that has so far had the highest conversion rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Epsilon-Greedy Algorithm</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/epislon_greedy.png\" width=\"75%\"/></center>\n",
    "\n",
    "a is action.  \n",
    "t is time.  \n",
    "$a^*$ is optimal action.  \n",
    "Îµ epsilon a parameter to choose is the probability that we explore.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What is the range for epsilon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Epsilon is a probability, thus bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Epsilon Default Value</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>.1 (or 10% exploration)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def epsilon_greedy(self, epsilon=0.1):\n",
    "    \"\"\"Pick a bandit uniformly at random epsilon percent of the time.\n",
    "    Otherwise pick the bandit with the best observed proportion of winning.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    pass\n",
    "```               \n",
    "\n",
    "Write the code without looking at other people's implementation.   \n",
    "It is okay to look at course notes and library documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(self, epsilon=0.1):\n",
    "    \"\"\"Pick a bandit uniformly at random epsilon percent of the time.\n",
    "    Otherwise pick the bandit with the best observed proportion of winning.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    if random.random() <= epsilon:\n",
    "        return np.random.randint(low=0, high=self.n_bandits)\n",
    "    else:\n",
    "        return np.argmax(self.wins / (self.trials+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(self, epsilon=0.1):\n",
    "    \"\"\"Pick a bandit uniformly at random epsilon percent of the time.\n",
    "    Otherwise pick the bandit with the best observed proportion of winning.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    if random.random() <= epsilon:\n",
    "        return np.random.randint(low=0, high=self.n_bandits) # Return a random choice\n",
    "    else:\n",
    "        return np.argmax(self.wins / (self.trials + 1)) # Return the current best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:      848     11      8      9\n",
      "Number of trials:    936     23     16     25\n",
      "Conversion rates:   0.91   0.48   0.50   0.36\n",
      "\n",
      "A total of 876 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=epsilon_greedy)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>A Common Real-world Design Pattern for Epsilon-Greedy </h2></center>\n",
    "    \n",
    "1. At the beginning, set epsilon high (.15-.25).\n",
    "1. After fix number of trials, lower (.05-.1).\n",
    "1. Keep low and fixed for remaining trails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is similar to a fixed schedule for learning rate decay for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are limitations of Epsilon-Greedy algorithm?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) At the beginning, we might select exploitation but do __not__ yet know which version is right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) If epsilon is .10, the epsilon-greedy algorithm will eventually show the best option 90% of the time. \n",
    "\n",
    "So this means that 10% of the time the algorithm will continue to randomly show different versions of the site, no matter how confident we are that a certain version is the best. There is a point where we should stop the exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Softmax Algorithm</h2></center>\n",
    "\n",
    "Choose the arm in proportion to its estimated value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Say $p_A$, $p_B$ and $p_C$ are the conversion rates for three versions of the site. We would choose site A with the following probability  \n",
    "\n",
    "$$ \\frac{exp(p_A/ \\tau)}{exp(p_A/ \\tau) + exp(p_B/ \\tau) + exp(p_C/ \\tau)} $$\n",
    "\n",
    "$\\tau$ is a chosen parameter that controls the ârandomnessâ of the choice, usually around 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def softmax(self, tau=0.01):\n",
    "    \"\"\"Pick an bandit according to the Boltzman Distribution.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    pass\n",
    "```                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(self, tau=0.01):\n",
    "    \"\"\"Pick an bandit according to the Boltzman Distribution.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "\n",
    "    # Make sure to play each bandit at least once\n",
    "    if len(self.trials.nonzero()[0]) < self.n_bandits:\n",
    "        return np.random.randint(0, self.n_bandits)\n",
    "\n",
    "    # Compute the terms of the softmax function\n",
    "    numerators = np.zeros(len(self.wins))\n",
    "    denom = 0\n",
    "    for i in range(len(self.wins)):\n",
    "        numerators[i] = math.exp((self.wins[i] / (self.trials[i]+1)) / tau)\n",
    "        denom += numerators[i]\n",
    "\n",
    "    # Compute the \"pmf\" of the bandits\n",
    "    last = 0\n",
    "    softmax_vals = np.zeros(len(self.wins))\n",
    "    for i in range(len(self.wins)):\n",
    "        softmax_vals[i] = last + (numerators[i] / denom)\n",
    "        last = softmax_vals[i]\n",
    "\n",
    "    # Pick a random number and see where it falls in the range\n",
    "    r = random.random()\n",
    "    for i in range(len(self.wins)):\n",
    "        if r <= softmax_vals[i]:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:        1    577      2      0\n",
      "Number of trials:      2    994      3      1\n",
      "Conversion rates:   0.50   0.58   0.67   0.00\n",
      "\n",
      "A total of 580 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=softmax)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Softmax Limitation</h2></center>\n",
    "\n",
    "It does __not__ take into account how many times an arm has been pulled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayesian Bandit (or Thompson Sampling)</h2></center>\n",
    "\n",
    "1. Calculate Posterior for each \"arm\" / action\n",
    "2. Pick a random sample from each Posterior\n",
    "3. Choose the arm with highest reward\n",
    "4. Update Posteriors for next round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayesian Bandit (or Thompson Sampling)</h2></center>\n",
    "\n",
    "<br>\n",
    "<center><img src=\"http://fastml.com/images/ab-testing/bandits_small.png\" width=\"50%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<center><a href=\"https://learnforeverlearn.com/bandits/\">Demo!</a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Thompson Sampling Steps</h2></center>\n",
    "\n",
    "1. For each k, sample $Î¸_k â¼ Ï(Î¸_k | Dt)$\n",
    "1. Then play bandit $k = arg max_k \\hatÎ¸_k$ \n",
    "1. Update played bandit win or lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Thompson Sampling for Bernoulli Bandits</h2></center>\n",
    "\n",
    "Given k bandits, each k pays off a reward of 1 with probability\n",
    "$Î¸_k$ and 0 with probability 1 â $Î¸_k$. \n",
    "\n",
    "Model each bandit as $Ï$. Parameterized as $Ï(Î¸_k | D_t)$\n",
    "\n",
    "Parameterization of the Beta family of distribution $Î¸ â¼ Beta(Î±, Î²)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Simple (but effective) Posterior is:\n",
    "\n",
    "$$Î¸_k | D_t â¼ Beta(1 + n_{k0}, 1 + n_{k1})$$\n",
    "\n",
    "\n",
    "Assume a uniform distribution is achieved using prior Î± = prior Î² = 1. \n",
    "\n",
    "- $\\alpha = 1 + $ number of times bandit has won\n",
    "- $\\beta = 1 + $ number of times bandit has lost  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Another sensible defaults is to use an uninformative prior by setting prior_alpha = prior_beta = 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Full Bayesian updating</h2></center>\n",
    "\n",
    "- Useful if you do not assume have stationary data\n",
    "- However, it is far more complicated and much slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Use the Bayesian Beta-Binomial conjugate prior techniques used to model the click-through rate as our base model for each of the bandits.  \n",
    "\n",
    "Sample from each banditâs distribution and play the bandit with the highest value.\n",
    "\n",
    "It will naturally converge on the bandit with the best payout.  \n",
    "\n",
    "[Source](http://fastml.com/ab-testing-with-bayesian-bandits-in-google-analytics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://davidrosenberg.github.io/mlcourse/in-prep/thompson-sampling-bernoulli.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def bayesian_bandit(self):\n",
    "    \"\"\"Randomly sample from a beta distribution for each bandit and pick the one\n",
    "    with the largest value.\n",
    "    Return the index of the winning bandit.\"\"\"    \n",
    "    pass\n",
    "```     \n",
    "\n",
    "Hints: \n",
    "\n",
    "- Use `scipy.stats.beta`\n",
    "- Seriously - use the all the methods on `scipy.stats.beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_bandit(self):\n",
    "    \"\"\"Randomly sample from a beta distribution for each bandit and pick the one\n",
    "    with the largest value.\n",
    "    Return the index of the winning bandit.\"\"\"    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_bandit(self):\n",
    "    \"\"\"Randomly sample from a beta distribution for each bandit and pick the one\n",
    "    with the largest value.\n",
    "    Return the index of the winning bandit.\"\"\"    \n",
    "    \n",
    "    # Make sure to play each bandit at least once\n",
    "    if len(self.trials.nonzero()[0]) < self.n_bandits:\n",
    "        return np.random.randint(0, self.n_bandits)\n",
    "\n",
    "    # Create a beta distribution for each bandit and draw a random sample\n",
    "    samples = np.zeros(len(self.wins))\n",
    "    for i in range(len(self.wins)):\n",
    "        samples[i] = stats.beta(1+self.wins[i], (1+self.trials[i]-self.wins[i])).rvs(1)\n",
    "    return np.argmax(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:      872      4      5     11\n",
      "Number of trials:    966      8      9     17\n",
      "Conversion rates:   0.90   0.50   0.56   0.65\n",
      "\n",
      "A total of 892 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=bayesian_bandit)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Simulate Results With The Implementations</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    926     28     25     21\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    241    247    271    241\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    246    249    250    255\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "p_array = [0]*4 # No winners\n",
    "# p_array = [1]*4 # All winners\n",
    "# p_array = [.5]*4 # All the same\n",
    "# p_array = [.01, .4, .8, .999] # All different\n",
    "\n",
    "strategies = [epsilon_greedy, softmax, bayesian_bandit]\n",
    "\n",
    "for strategy in strategies:\n",
    "    b = Bandit(p_array=p_array,\n",
    "                choice_function=strategy)\n",
    "    b.sample_bandits(n=1_000) # 1000 is a good value\n",
    "    print()\n",
    "    print('#'*50)\n",
    "    print()\n",
    "    print(strategy.__name__.title())\n",
    "    print_results(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        7      2      3    743\n",
      "Number of trials:    107     29     22    842\n",
      "Conversion rates:   0.07   0.07   0.14   0.88\n",
      "\n",
      "A total of 755 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        0      0      0    881\n",
      "Number of trials:      3      1      1    995\n",
      "Conversion rates:   0.00   0.00   0.00   0.89\n",
      "\n",
      "A total of 881 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        0      0      0    882\n",
      "Number of trials:      3      3      3    991\n",
      "Conversion rates:   0.00   0.00   0.00   0.89\n",
      "\n",
      "A total of 882 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:        7     87      4      2\n",
      "Number of trials:    110    822     42     26\n",
      "Conversion rates:   0.06   0.11   0.10   0.08\n",
      "\n",
      "A total of 100 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:        0     87      0      0\n",
      "Number of trials:      4    991      4      1\n",
      "Conversion rates:   0.00   0.09   0.00   0.00\n",
      "\n",
      "A total of 87 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:       22     17     38      9\n",
      "Number of trials:    270    179    399    152\n",
      "Conversion rates:   0.08   0.09   0.10   0.06\n",
      "\n",
      "A total of 86 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        2      5     13    425\n",
      "Number of trials:     26     28     31    915\n",
      "Conversion rates:   0.08   0.18   0.42   0.46\n",
      "\n",
      "A total of 445 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        0      0      0    477\n",
      "Number of trials:      1      2      1    996\n",
      "Conversion rates:   0.00   0.00   0.00   0.48\n",
      "\n",
      "A total of 477 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        2     17     12    445\n",
      "Number of trials:     15     52     39    894\n",
      "Conversion rates:   0.13   0.33   0.31   0.50\n",
      "\n",
      "A total of 476 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "probs ={'atlantic_city': [0.1, 0.1, 0.1, 0.90],\n",
    "        'las_vegas':     [0.1, 0.1, 0.1, 0.12],\n",
    "        'reno':          [0.1, 0.2, 0.4, 0.50]}\n",
    "\n",
    "for prob in  probs.items():\n",
    "    p_array = prob[1]\n",
    "    for strategy in strategies:\n",
    "        b = Bandit(p_array=p_array,\n",
    "                    choice_function=strategy)\n",
    "        b.sample_bandits(n=1_000) # 1000 is a good value\n",
    "        print()\n",
    "        print('#'*50)\n",
    "        print()\n",
    "        print(strategy.__name__.title())\n",
    "        print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Which MAB strategy should I use?</h2></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bandit_choice.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/map.jpeg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Challenge Question</h2></center>\n",
    "\n",
    "Think about putting this into production. What kind of testing would you have to do (unit, integration, acceptance)? How would you monitor the different choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Epsilon-Greedy Algorithm sometimes pick randomly, other times pick the best.\n",
    "- Softmax is structured exploration.\n",
    "- Bayesian Bandit Algorithm models what we have seen (and uncertainty). Randomly sample, then pick best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bonus Materials</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which strategy should I use?\n",
    "------\n",
    "<center><img src=\"images/compare.png\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3) Upper Confidence Bound (UCB)\n",
    "-----\n",
    "\n",
    "Choose the strategy that where the following value is the largest:  \n",
    "\n",
    "$$ p_A + \\sqrt{\\frac{2 log(N)}{n_A}} $$  \n",
    "\n",
    "- $p_A$ is the conversion rate for that version so far  \n",
    "- where $N$ is the total number of rounds (for all sites) \n",
    "- $n_A$ is the number of times site A has been shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Upper Confidence Bound (UCB)\n",
    "-----\n",
    "\n",
    "Proven to have a logarithmic upper bound for regret (hence its name). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You should first make sure to play each bandit once so none of the $n_A$'s are 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Upper Confidence Bound (UCB) Features\n",
    "-----\n",
    "\n",
    "* UCB does __not__ use randomness at all. Unlike epsilon-greedy, itâs possible to know exactly how UCB will behave in any given situation. This can make it easier to reason about at times. \n",
    "\n",
    "* UCB does __not__ have free parameters that you need to configure before you can deploy it. This is a major improvement if youâre interested in running in the wild, because it means that you can start UCB without having clear sense of how you expect the world to behave "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb1(self):\n",
    "    \"\"\"Pick the bandit according to the UCB1 strategy.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    # Make sure to play each bandit at least once\n",
    "    if len(self.trials.nonzero()[0]) < self.n_bandits:\n",
    "        return np.random.randint(0, self.n_bandits)\n",
    "\n",
    "    # Compute the UCB1 values for each bandit\n",
    "    ucb1_vals = np.zeros(len(self.wins))\n",
    "    for i in range(len(self.wins)):\n",
    "        ucb1_vals[i] = (self.wins[i] \n",
    "                        / (self.trials[i]+1.)\n",
    "                        + math.sqrt(2.*math.log(self.N) / self.trials[i]))\n",
    "\n",
    "    # return the max\n",
    "    return np.argmax(ucb1_vals)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Study\n",
    "------\n",
    "\n",
    "[Boltzmann Exploration Done Right](https://papers.nips.cc/paper/7208-boltzmann-exploration-done-right.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
