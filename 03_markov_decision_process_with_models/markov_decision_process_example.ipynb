{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Markov-Decision-Process-(MDP)\" data-toc-modified-id=\"Markov-Decision-Process-(MDP)-1\">Markov Decision Process (MDP)</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#By-the-end-of-this-session,-you-should-be-able-to:\" data-toc-modified-id=\"By-the-end-of-this-session,-you-should-be-able-to:-2.0.1\">By the end of this session, you should be able to:</a></span></li></ul></li></ul></li><li><span><a href=\"#Brownie---The-Warehouse-Robot\" data-toc-modified-id=\"Brownie---The-Warehouse-Robot-3\">Brownie - The Warehouse Robot</a></span></li><li><span><a href=\"#Markov-Decision-Process-(MDP)\" data-toc-modified-id=\"Markov-Decision-Process-(MDP)-4\">Markov Decision Process (MDP)</a></span></li><li><span><a href=\"#Brownie's-Warehouse\" data-toc-modified-id=\"Brownie's-Warehouse-5\">Brownie's Warehouse</a></span></li><li><span><a href=\"#Brownie's-Warehouse\" data-toc-modified-id=\"Brownie's-Warehouse-6\">Brownie's Warehouse</a></span></li><li><span><a href=\"#Markov-Decision-Process-(MDP)\" data-toc-modified-id=\"Markov-Decision-Process-(MDP)-7\">Markov Decision Process (MDP)</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-8\">Check for understanding</a></span></li><li><span><a href=\"#As-Brownie's-Managers,-we-incentivize-Brownie-to-be-efficient\" data-toc-modified-id=\"As-Brownie's-Managers,-we-incentivize-Brownie-to-be-efficient-9\">As Brownie's Managers, we incentivize Brownie to be efficient</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-10\">Check for understanding</a></span></li><li><span><a href=\"#Stochastic-Process\" data-toc-modified-id=\"Stochastic-Process-11\">Stochastic Process</a></span></li><li><span><a href=\"#MDP's-5-Elements\" data-toc-modified-id=\"MDP's-5-Elements-12\">MDP's 5 Elements</a></span></li><li><span><a href=\"#MDP's-5-Elements\" data-toc-modified-id=\"MDP's-5-Elements-13\">MDP's 5 Elements</a></span></li><li><span><a href=\"#P---The-Transition-Probability-Function.-\" data-toc-modified-id=\"P---The-Transition-Probability-Function.--14\">P - The Transition Probability Function. </a></span></li><li><span><a href=\"#MDP's-5-Elements\" data-toc-modified-id=\"MDP's-5-Elements-15\">MDP's 5 Elements</a></span></li><li><span><a href=\"#γ---The-discount-factor\" data-toc-modified-id=\"γ---The-discount-factor-16\">γ - The discount factor</a></span></li><li><span><a href=\"#RL-goal:-The-agent-maximizes-its-expected-discounted-return\" data-toc-modified-id=\"RL-goal:-The-agent-maximizes-its-expected-discounted-return-17\">RL goal: The agent maximizes its expected discounted return</a></span></li><li><span><a href=\"#MDP's-5-Elements\" data-toc-modified-id=\"MDP's-5-Elements-18\">MDP's 5 Elements</a></span></li><li><span><a href=\"#Questions?\" data-toc-modified-id=\"Questions?-19\">Questions?</a></span></li><li><span><a href=\"#π---policy\" data-toc-modified-id=\"π---policy-20\">π - policy</a></span></li><li><span><a href=\"#π*---Optimal-Policy\" data-toc-modified-id=\"π*---Optimal-Policy-21\">π<sup>*</sup> - Optimal Policy</a></span></li><li><span><a href=\"#Optimal-Policy-Equation\" data-toc-modified-id=\"Optimal-Policy-Equation-22\">Optimal Policy Equation</a></span></li><li><span><a href=\"#Finite-vs-Infinite-Horizons\" data-toc-modified-id=\"Finite-vs-Infinite-Horizons-23\">Finite vs Infinite Horizons</a></span></li><li><span><a href=\"#π*s-as-optimal-policy-\" data-toc-modified-id=\"π*s-as-optimal-policy--24\">π<sup>*</sup><sub>s</sub> as optimal policy </a></span></li><li><span><a href=\"#What-does-it-mean-to-be-Markov-in-a-Markov-decision-process-(MDP)?\" data-toc-modified-id=\"What-does-it-mean-to-be-Markov-in-a-Markov-decision-process-(MDP)?-25\">What does it mean to be Markov in a Markov decision process (MDP)?</a></span></li><li><span><a href=\"#Markov-Assumption\" data-toc-modified-id=\"Markov-Assumption-26\">Markov Assumption</a></span></li><li><span><a href=\"#Rewards-vs-Utility\" data-toc-modified-id=\"Rewards-vs-Utility-27\">Rewards vs Utility</a></span></li><li><span><a href=\"#Utility-for-each-state\" data-toc-modified-id=\"Utility-for-each-state-28\">Utility for each state</a></span></li><li><span><a href=\"#Bellman-equation\" data-toc-modified-id=\"Bellman-equation-29\">Bellman equation</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-30\">Check for understanding</a></span></li><li><span><a href=\"#Bellman-Equation-FTW-(For-The-Win)\" data-toc-modified-id=\"Bellman-Equation-FTW-(For-The-Win)-31\">Bellman Equation FTW (For The Win)</a></span></li><li><span><a href=\"#Value-Iteration\" data-toc-modified-id=\"Value-Iteration-32\">Value Iteration</a></span></li><li><span><a href=\"#Value-Iteration-&amp;-Bellman-Equation\" data-toc-modified-id=\"Value-Iteration-&amp;-Bellman-Equation-33\">Value Iteration &amp; Bellman Equation</a></span></li><li><span><a href=\"#Putting-the-Iterative-in-Value-Iteration\" data-toc-modified-id=\"Putting-the-Iterative-in-Value-Iteration-34\">Putting the Iterative in Value Iteration</a></span></li><li><span><a href=\"#Value-Iteration-Pseudocode\" data-toc-modified-id=\"Value-Iteration-Pseudocode-35\">Value Iteration Pseudocode</a></span></li><li><span><a href=\"#Value-Iteration-Pseudocode----Alternative-Version\" data-toc-modified-id=\"Value-Iteration-Pseudocode----Alternative-Version-36\">Value Iteration Pseudocode -  Alternative Version</a></span></li><li><span><a href=\"#Y'all-be-implementing-Value-Iteration-for-this-week's-lab.\" data-toc-modified-id=\"Y'all-be-implementing-Value-Iteration-for-this-week's-lab.-37\">Y'all be implementing Value Iteration for this week's lab.</a></span></li><li><span><a href=\"#Questions?\" data-toc-modified-id=\"Questions?-38\">Questions?</a></span></li><li><span><a href=\"#Optimal-Paths-vs.-Optimal-Policy\" data-toc-modified-id=\"Optimal-Paths-vs.-Optimal-Policy-39\">Optimal Paths vs. Optimal Policy</a></span></li><li><span><a href=\"#Optimal-Policy-For-Each-State\" data-toc-modified-id=\"Optimal-Policy-For-Each-State-40\">Optimal Policy For Each State</a></span></li><li><span><a href=\"#Recall,--Brownie-Moves-Stochastically\" data-toc-modified-id=\"Recall,--Brownie-Moves-Stochastically-41\">Recall,  Brownie Moves Stochastically</a></span></li><li><span><a href=\"#What-happens-if-we-changed-the-baseline-reward-(R)-for-each-move?-\" data-toc-modified-id=\"What-happens-if-we-changed-the-baseline-reward-(R)-for-each-move?--42\">What happens if we changed the baseline reward (R) for each move? </a></span></li><li><span><a href=\"#MDP:-Beyond-The-Grid\" data-toc-modified-id=\"MDP:-Beyond-The-Grid-43\">MDP: Beyond The Grid</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-44\">Summary</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-45\">Summary</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-46\">Check for understanding</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-47\">Check for understanding</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-48\">Bonus Material</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov Decision Process (MDP)</h2></center>\n",
    "\n",
    "<center><img src=\"https://images.deepai.org/django-summernote/2019-03-19/c8c9f96b-cc21-4d33-8b37-cb810f599e6e.png\" width=\"95%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "#### By the end of this session, you should be able to:\n",
    "\n",
    "- Frame Reinforcement Learning problems as a Markov decision process (MDP).\n",
    "- Define the 5 elements of MDP.\n",
    "- Calculate the utility of different states with the Bellman equation.\n",
    "- Pick the optimal policy for a MDP based on those calculated utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Brownie - The Warehouse Robot</h2></center>\n",
    "<center><img src=\"images/robot-control-warehouse.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Brownie needs to choose the best path, even though might not have all the information</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov Decision Process (MDP)</h2></center>\n",
    "\n",
    "A mathematical framework for modeling decision-making in under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Well, partially uncertainty - where outcomes are partly random and partly under the control of an agent.\n",
    "\n",
    "MDPs are __not__ appropriate if:\n",
    "\n",
    "- State-Action pairs are completely deterministic.\n",
    "- Rewards are completely stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Brownie's Warehouse</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_blank.png\" width=\"65%\"/></center>\n",
    "\n",
    "<center>Model-based learning - We are going to compute a model of the world (i.e., the transitions and rewards). </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Brownie's Warehouse</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_blank.png\" width=\"25%\"/></center> \n",
    "\n",
    "- Brownie always starts in (0,0) \n",
    "- Brownie makes a finite sequence of discrete steps.\n",
    "- Possible actions are: Up, Down, Left, and Right (aka Pac-man actions).\n",
    "- The end (aka, terminal) states are (3, 2) with +1 and (3, 1) with -1.\n",
    "- Assume the environment is fully observable.\n",
    "- Brownie wants to do a good job and maximize cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov Decision Process (MDP)</h2></center>\n",
    "\n",
    "<center><img src=\"https://images.deepai.org/django-summernote/2019-03-19/c8c9f96b-cc21-4d33-8b37-cb810f599e6e.png\" width=\"75%\"/></center>\n",
    "\n",
    "At each time step, the agent is in some state $s$ in the environment.\n",
    "\n",
    "The agent may choose any action $a$ that is available in state $s$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_blank.png\" width=\"30%\"/></center>\n",
    "\n",
    "What the states? How many states are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The states are grid-world cells within the warehouse.\n",
    "\n",
    "There are 11 valid states - 4*3-1 (1 state is unreachable because of a support column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>As Brownie's Managers, we incentivize Brownie to be efficient</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"40%\"/></center>\n",
    "\n",
    "<center>There will be a small negative reward for each move.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What are the optimal paths (i.e., series of actions) to maximize cumulative rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Brownie's optimal paths could be:\n",
    "\n",
    "1. Up, Up, Right, Right, Right\n",
    "1. Right, Right, Up, Up, Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Stochastic Process</h2></center>\n",
    "<center><img src=\"images/tranisition_model.png\" width=\"30%\"/></center>\n",
    "\n",
    "However, Brownie is in training and moves non-deterministically.\n",
    "\n",
    "Each action achieves intended outcome with probability 0.8. \n",
    "\n",
    "But there is probability 0.2 that Brownie moves at right angles to the intended direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"30%\"/></center>\n",
    "\n",
    "If in(0,0) and the intended action is Up, the result could be:\n",
    "\n",
    "- With probability 0.8, moves Up to (0,1) \n",
    "- With probability 0.1, moves Right to (1,0) \n",
    "- With with probability 0.1, moves Left, bumps into the wall, and stays in (0,0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MDP's 5 Elements</h2></center>\n",
    "\n",
    "<center>(S,A,P,R,γ)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) S - the state space, the set of all unique situations the agent could be in. \n",
    "\n",
    "We are going to assume finite, discrete states.\n",
    "\n",
    "S<sub>0</sub> is the initial state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) A - the action space, the set of all actions available to the agent.\n",
    "\n",
    "We are going to assume finite, discrete actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MDP's 5 Elements</h2></center>\n",
    "\n",
    "<center>(S,A,P,R,γ)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) P - the transition probability function. \n",
    "\n",
    "$$P(s′ |s, a)$$\n",
    "\n",
    "The probability of transition to another state s′ when given action a is taken in current s. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>P - The Transition Probability Function. </h2></center>\n",
    "\n",
    "$$P(s′ |s, a)$$\n",
    "\n",
    "<center><img src=\"images/tree_of_state.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$P(s′ |s, a)$ as a three-dimensional table containing probabilities for outcome for each state-action pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MDP's 5 Elements</h2></center>\n",
    "\n",
    "<center>(S,A,P,R,γ)</center>\n",
    "\n",
    "4) R - the reward function\n",
    "\n",
    "Returns the rewards obtained when action a is taken from s and the following state s′."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5) γ - the discount factor. Determines how much influence the subsequent rewards have on the value of a particular state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>γ - The discount factor</h2></center>\n",
    "\n",
    "- Quantifies the difference between actual immediate rewards and possible future rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A real-valued number between 0 and 1. \n",
    "- When γ is close to 0, rewards in the distant future are viewed as insignificant. \n",
    "- When γ is 1, future rewards are exactly equivalent to current rewards.\n",
    "- Discounting is a good model of human preferences over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>RL goal: The agent maximizes its expected discounted return</h2></center>\n",
    "\n",
    "$$R_t = \\sum_{k=0}^∞ γ^k r_{t+k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MDP's 5 Elements</h2></center>\n",
    "<center>(S,A,P,R,γ)</center>\n",
    "\n",
    "1. State\n",
    "2. Action\n",
    "3. Probabilities of Transition \n",
    "4. Reward\n",
    "5. Discounting rate of future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Questions?</h2></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>π - policy</h2></center>\n",
    "\n",
    "Policy is a way of acting. \n",
    "\n",
    "Policy is a mapping between state and actions.\n",
    "\n",
    "$π(s)$ is the action recommended by the policy $π$ for state $s$. \n",
    "\n",
    "Given the current state, what should the agent should try to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>π<sup>*</sup> - Optimal Policy</h2></center>\n",
    "\n",
    "An optimal policy ($π^∗$) always chooses the action that maximizes the expected return for the given, current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Optimal Policy Equation</h2></center>\n",
    "\n",
    "$$π^*(s) = argmax \\sum_{s′}P(s′ | s,a)U(s′)$$\n",
    "\n",
    "The optimal policy is the policy that is mostly likely to be successful, weighted by utility/reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Finite vs Infinite Horizons</h2></center>\n",
    "\n",
    "A finite horizon means a fixed number of time steps (N) after which nothing matters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Infinite horizon means we can take as many steps as we need to accomplish the task.  \n",
    "\n",
    "We still are penalized for each step, but there is no no fixed deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's assume infinite horizons for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>π<sup>*</sup><sub>s</sub> as optimal policy </h2></center>\n",
    "\n",
    "$π^*_s$ is a policy that recommends an action for every state.\n",
    "\n",
    "With discounted future rewards and infinite horizons, an optimal policy is independent of the starting state.\n",
    "\n",
    "In other words - if we play the game until it ends, we can find an  optimal policy from any specific state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What does it mean to be Markov in a Markov decision process (MDP)?</h2></center>\n",
    "\n",
    "Once Brownie is in a state, it does not matter how it got there.\n",
    "\n",
    "To maximize cumulative rewards, we only need to know current state and optimal State-Action pairs (i.e., policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov Assumption</h2></center>\n",
    "\n",
    "<center><img src=\"images/markov_quote.png \" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Rewards vs Utility</h2></center>\n",
    "\n",
    "R(s) is the “short term” reward for being in a state.\n",
    "\n",
    "U(s) is the “long term” total reward from state onward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Utility for each state</h2></center>\n",
    "\n",
    "<center><img src=\"images/utility.png\" width=\"30%\"/></center>\n",
    "\n",
    "Utilities are higher for states closer to the +1 exit, because fewer steps are required to reach the successful exit.\n",
    "\n",
    "Utility is lowest in the bottom right corner because it requires the most steps to successful exit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Assuming γ=1 and R(s) = − 0.04 for nonterminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bellman equation</h2></center>\n",
    "\n",
    "$$U(s) = R(s) + γ • max \\sum_{s′}P(s′ | s,a)U(s′)$$\n",
    "\n",
    "The utility of a state is the immediate reward for that state plus the expected discounted utility of the next state, assuming that the agent chooses the optimal action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "$$U(s) = R(s) + γ max \\sum_{s′}P(s′ | s,a)U(s′)$$\n",
    "\n",
    "What is the calculated utility for S <sub>0</sub>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assume γ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we assume infinite horizon, γ = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "R(s) = -0.04\n",
    "\n",
    "Possible future states = {Up, Left, Down, Right}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Up = 0.8*U(0,1) + 0.1*U(1,0) +0.1U(0,0)    \n",
    "Up = `0.8*.762 + 0.1*.655 +0.1*.705`   \n",
    "Up = 0.7456  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Left = 0.9*U(0, 0) + 0.1*U(0, 1)  \n",
    "Left = 0.9*.705 + 0.1*.762  \n",
    "Left = 0.711  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Down = 0.9*U(0, 0) + 0.1*U (1, 0)   \n",
    "Down = 0.9*.705 + 0.1*.655  \n",
    "Down = 0.7  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Right = 0.8U (1, 0) + 0.1U (0, 1) + 0.1U (0, 0)   \n",
    "Right = 0.8*.655 + 0.1*.762  + 0.1*.705   \n",
    "Right = 0.671  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " U(s) = −0.04 + 1* max(Up, Left, Down, Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7056"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The expect utility of being in the start state\n",
    "-0.04 + max([0.7456, 0.711, 0.7, 0.671 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/utility.png\" width=\"30%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bellman Equation FTW (For The Win)</h2></center>\n",
    "\n",
    "The solution to the Bellman equation is the optimal policy.\n",
    "\n",
    "Brownie should try to move Up because that has the maximum utility from starting cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Iteration</h2></center>\n",
    "\n",
    "Calculate the utility of each state. \n",
    "\n",
    "Then use those calculated state utilities to select an optimal action in each state.\n",
    "\n",
    "The Bellman equation is the basis of the Value Iteration algorithm for solving MDPs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Iteration & Bellman Equation</h2></center>\n",
    "\n",
    "If there are $n$ possible states, then there are $n$ Bellman equations (one for each state).\n",
    "\n",
    "The $n$ equations contain $n$ unknowns (the utilities of the states). \n",
    "\n",
    "We want to solve these simultaneous equations to calculate the utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do you solve n simultaneous equations n unknowns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Linear algebra!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But wait, there is a problem - the equations are nonlinear, because the “max” operator is __not__ a linear operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Putting the Iterative in Value Iteration</h2></center>\n",
    "\n",
    "1. Start with arbitrary initial values for the utilities.\n",
    "1. Calculate the right-hand side of the equation.\n",
    "1. Plug it into the left-hand side - thereby updating the utility of each state from the utilities of its neighbors.\n",
    "1. Repeat step 2 & 3 until we reach an equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Iteration Pseudocode</h2></center>\n",
    "<center><img src=\"images/value_iteration_v1.png\" width=\"80%\"/></center>\n",
    "θ is generic. ε is specific for tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source - Sutton & Barto Section 4.4 p83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Value Iteration Pseudocode -  Alternative Version</h2></center>\n",
    "<center><img src=\"images/value_iteration_v2.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Y'all be implementing Value Iteration for this week's lab.</h2></center>\n",
    "\n",
    "<center><h2>Questions?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Optimal Paths vs. Optimal Policy</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"30%\"/></center>\n",
    "\n",
    "Brownie's optimal paths could be:\n",
    "\n",
    "1. Up, Up, Right, Right, Right\n",
    "1. Right, Right, Up, Up, Right\n",
    "\n",
    "For Brownie, what is $π^*$ for S<sub>0</sub>? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"30%\"/></center>\n",
    "\n",
    "Given starting in (0,0), we'll recommend Brownie to follow the optimal policy o taking optimal path #1: Up, Up, Right, Right, Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Optimal Policy For Each State</h2></center>\n",
    "\n",
    "<center><img src=\"images/optimal_policy_warehouse.png\" width=\"40%\"/></center>\n",
    "\n",
    "<center>There is a huge penalty for ending up in (3,1) by accident so go the long way!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Recall,  Brownie Moves Stochastically</h2></center>\n",
    "\n",
    "<center><img src=\"images/oops.jpg\" width=\"30%\"/></center>\n",
    "\n",
    "What is the probability that Brownie will perfectly follow the optimal policy (Up, Up, Right, Right, Right) from S<sub>0</sub> and reach (3,2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.77%\n"
     ]
    }
   ],
   "source": [
    "n_moves = 5\n",
    "probability_of_optimal_execution = .8**n_moves\n",
    "print(f\"{probability_of_optimal_execution:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " We'll need to monitor Brownie during the entire path to make sure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What happens if we changed the baseline reward (R) for each move? </h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"25%\"/></center>\n",
    "\n",
    "What is the optimal policy, if:\n",
    "- $R$ is very negative.\n",
    "- $R$ is gradually less negative.\n",
    "- $R$ is positive.\n",
    "- What range of values would our current optimal policy remain optimal?\n",
    "\n",
    "Think, Pair, Share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/r_very_neg.png\" width=\"50%\"/></center>\n",
    "\n",
    "When R(s) ≤ −1.6284, work is so painful that the Brownies heads straight for the nearest finish.\n",
    "\n",
    "Even if the finish is worth –1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/r_kinda_neg.png\" width=\"50%\"/></center>\n",
    "\n",
    "When −0.4278 ≤ R(s) ≤ −0.0850, work is quite unpleasant.\n",
    "\n",
    "Brownie always takes the shortest route to the +1 state.\n",
    "\n",
    "In particular, the agent will the take the dangerous shortcut and is willing to risk falling into the –1 state by accident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/r_little_negative.png\" width=\"50%\"/></center>\n",
    "\n",
    "When work is only slightly dreary (−0.0221 < R(s) < 0), the optimal policy is taking no risks at all. \n",
    "\n",
    "The agent will always heads directly away from the –1 state so that it cannot fall in by accident, even though this means banging its head against the column or wall quite a few times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/r_positive.png\" width=\"50%\"/></center>\n",
    "\n",
    "If R(s) > 0, just cruising the warehouse is positively enjoyable. \n",
    "\n",
    "The agent avoids actively avoids picking up both packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"https://www.azquotes.com/picture-quotes/quote-show-me-the-incentive-and-i-will-show-you-the-outcome-charlie-munger-138-85-72.jpg\" width=\"100%\"/></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MDP: Beyond The Grid</h2></center>\n",
    "\n",
    "<center><img src=\"images/mdp1.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Summary</h2></center>\n",
    "\n",
    "- Markov Decision Process (MDP) is the most common Reinforcement Learning framework.\n",
    "- MDP's has 5 Elements (S,A,P,R,γ):\n",
    "    1. S - State space\n",
    "    1. A - Action space\n",
    "    1. P - Transition probability function\n",
    "    1. R - Reward function\n",
    "    1. γ - Discount factor \n",
    "- γ - Discount factor\n",
    "    - 0 - Maximizes current observed rewards\n",
    "    - 1 - Future possible rewards are as valuable as current rewards\n",
    "- $π$ is a policy, a mapping from state to action.\n",
    "- $π^*$ is the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Summary</h2></center>\n",
    "\n",
    "- The Bellman equation can calculate the expected utility, thus picking optimal policy.\n",
    "- Value iteration solves the Bellman equation by repetitively solving the equation until equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "For a given state, what is $\\sum_{a} π(s, a)$ equal?\n",
    "\n",
    "What is that equation in your own words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\sum_{a} π(s, a)$ = 1\n",
    "\n",
    "In a given state, there is a set of actions. Policies must select among them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What the relationship between multi-arm bandits (MAB) and MDPs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "MAB only has a single state, the agent starts and ends in the same state.\n",
    "\n",
    "You think of a physical location. The agent is in a always in the same location.\n",
    "\n",
    "\n",
    "Each arm is an action with a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maximizing cumulative rewards is the goal of RL / MDP.  \n",
    "Minimizing regret is the goal of MAB.\n",
    "\n",
    "They are the very similar:\n",
    "Maximize cumulative reward = minimize total regret "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/discounted_rewards.png\" width=\"75%\"/></center>\n",
    " \n",
    " Let’s look at this sum term by term. First of all, we’re summing across all time steps t. Let’s set γ at 1 for now and forget about it. r(x,a) is a reward function. For state x and action a (i.e., go left at a crossroads) it gives you the reward associated with taking that action a at state x. Going back to our equation, we’re trying to maximize the sum of future rewards by taking the best action in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This lesson is an adaption of of Chapter 17 of [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov Decision Processes (MDPs) Environment and Actions\n",
    "\n",
    "- Fully Observable (Chess) vs Partially Observable (Poker)\n",
    "- Deterministic (Cart Pole) vs Stochastic (DeepTraffic)\n",
    "- Static (Chess) vs Dynamic (DeepTraffic)\n",
    "- Discrete (Chess) vs Continuous (Cart Pole)\n",
    "- Single Agent (Atari) vs Multi Agent (DeepTraffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goal of this (like all computational fields) to create a simplified model of the world that computational efficient.\n",
    "\n",
    "The real-world is complex\n",
    "\n",
    "Reinforcement Learning models in a simpler, very specific form.\n",
    "\n",
    "Markov {Chain, Reward, Decision} Process is a simpler, very specific form\n",
    "\n",
    "Then we can apply our set of algorithms\n",
    "\n",
    "(very similar to modeling the world with Probability Theory)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
