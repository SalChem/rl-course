{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Value-Function-Approximation\" data-toc-modified-id=\"Value-Function-Approximation-1\">Value Function Approximation</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#By-the-end-of-this-session,-you-should-be-able-to:\" data-toc-modified-id=\"By-the-end-of-this-session,-you-should-be-able-to:-2.0.1\">By the end of this session, you should be able to:</a></span></li></ul></li></ul></li><li><span><a href=\"#Review\" data-toc-modified-id=\"Review-3\">Review</a></span></li><li><span><a href=\"#Review\" data-toc-modified-id=\"Review-4\">Review</a></span></li><li><span><a href=\"#Q-learning-Steps\" data-toc-modified-id=\"Q-learning-Steps-5\">Q-learning Steps</a></span></li><li><span><a href=\"#Q-learning-Steps\" data-toc-modified-id=\"Q-learning-Steps-6\">Q-learning Steps</a></span></li><li><span><a href=\"#Game-of-the-Day:-ASCII-Breakout\" data-toc-modified-id=\"Game-of-the-Day:-ASCII-Breakout-7\">Game of the Day: ASCII Breakout</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-8\">Check for understanding</a></span></li><li><span><a href=\"#What-is-tabular-Q-learning?\" data-toc-modified-id=\"What-is-tabular-Q-learning?-9\">What is tabular Q-learning?</a></span></li><li><span><a href=\"#The-major-limitation-of-Q-learning\" data-toc-modified-id=\"The-major-limitation-of-Q-learning-10\">The major limitation of Q-learning</a></span></li><li><span><a href=\"#State-Aggregation\" data-toc-modified-id=\"State-Aggregation-11\">State Aggregation</a></span></li><li><span><a href=\"#Symmetry-as-Quantization\" data-toc-modified-id=\"Symmetry-as-Quantization-12\">Symmetry as Quantization</a></span></li><li><span><a href=\"#Value-Function-Approximation-(VFA)\" data-toc-modified-id=\"Value-Function-Approximation-(VFA)-13\">Value Function Approximation (VFA)</a></span></li><li><span><a href=\"#Value-Function-Approximation-(VFA)\" data-toc-modified-id=\"Value-Function-Approximation-(VFA)-14\">Value Function Approximation (VFA)</a></span></li><li><span><a href=\"#Linear-Regression-Review\" data-toc-modified-id=\"Linear-Regression-Review-15\">Linear Regression Review</a></span></li><li><span><a href=\"#Linear-Regression-Review\" data-toc-modified-id=\"Linear-Regression-Review-16\">Linear Regression Review</a></span></li><li><span><a href=\"#What-is-the-goal-of-Machine-Learning-(ML)?\" data-toc-modified-id=\"What-is-the-goal-of-Machine-Learning-(ML)?-17\">What is the goal of Machine Learning (ML)?</a></span></li><li><span><a href=\"#-The-true-power-of-Value-Function-Approximation-is-Generalization.\" data-toc-modified-id=\"-The-true-power-of-Value-Function-Approximation-is-Generalization.-18\"> The true power of Value Function Approximation is Generalization.</a></span></li><li><span><a href=\"#Value-Function-Approximation-(VFA)-Formalism\" data-toc-modified-id=\"Value-Function-Approximation-(VFA)-Formalism-19\">Value Function Approximation (VFA) Formalism</a></span></li><li><span><a href=\"#Value-Function-Approximation-(VFA)\" data-toc-modified-id=\"Value-Function-Approximation-(VFA)-20\">Value Function Approximation (VFA)</a></span></li><li><span><a href=\"#Value-Function-Approximation-(VFA)\" data-toc-modified-id=\"Value-Function-Approximation-(VFA)-21\">Value Function Approximation (VFA)</a></span></li><li><span><a href=\"#Questions-about-VFA?\" data-toc-modified-id=\"Questions-about-VFA?-22\">Questions about VFA?</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-23\">Check for understanding</a></span></li><li><span><a href=\"#(An-incomplete-list-of)-Possible-Functions-for-VFA\" data-toc-modified-id=\"(An-incomplete-list-of)-Possible-Functions-for-VFA-24\">(An incomplete list of) Possible Functions for VFA</a></span></li><li><span><a href=\"#-Life-is-too-short-for-Linear-Regression-as-a-VFA\" data-toc-modified-id=\"-Life-is-too-short-for-Linear-Regression-as-a-VFA-25\"> Life is too short for Linear Regression as a VFA</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-26\">Check for understanding</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-27\">Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Function Approximation</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/warren.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.chicagotribune.com/news/ct-xpm-1994-09-11-9409110413-story.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "#### By the end of this session, you should be able to:\n",
    "\n",
    "- List the major limitations of tabular Q-learning.\n",
    "- Explain how Value Function Approximation can overcome those limits.\n",
    "- Compare and contrast the common methods of Value Function Approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Review</h2></center>\n",
    "\n",
    "Q-learning updates a table of \\_\\_\\_\\_\\_\\_\\_\\_ which are the \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ if the agent performs action $a$ in state $s$.\n",
    "\n",
    "The states (and their rewards) are discovered during learning (aka, \\_\\_\\_\\_\\_\\_\\_\\_).\n",
    "\n",
    "If we always choose the action with the maximum reward, we might not find other states. Thus, we need to need to \\_\\_\\_\\_\\_\\_\\_\\_. \n",
    "\n",
    "Q-learning can update the value table with a better reward estimate even though the action might not be associated with the strategy the agent is following (aka, \\_\\_\\_\\_\\_\\_\\_).\n",
    "\n",
    "Q-learning in non-mathematical notion:   \n",
    "The updated value is a combination of the \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ plus the weighted amount of \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ and the estimated \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ if we act in our best estimated optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Review</h2></center>\n",
    "\n",
    "Q-learning updates a table of Q-values which are the expected discounted reward if the agent performs action $a$ in state $s$.\n",
    "\n",
    "The states (and their rewards) are discovered during learning (aka, model-free).\n",
    "\n",
    "If we always choose the action with the maximum reward, we might not find other states. Thus, we need to need to explore. \n",
    "\n",
    "Q-learning can update the value table with a better reward estimate even though the action might not be associated with the strategy the agent is following (aka, off-policy).\n",
    "\n",
    "Q-learning in non-mathematical notion:   \n",
    "The updated value is a combination of the previous value plus the weighted amount of reward and the estimated optimal discounted future values if we act in our best estimated optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Steps</h2></center>\n",
    "\n",
    "1. Measure Reward\n",
    "1. Initialize Q\n",
    "1. Perform action\n",
    "1. Update Q\n",
    "1. Choose action from Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Somehow this slide got mixed upü§¶‚Äç‚ôÇÔ∏è Please put these Q-learning steps in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Steps</h2></center>\n",
    "\n",
    "<center><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/12042140/11038f3.jpg\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Game of the Day: ASCII Breakout</h2></center>\n",
    "\n",
    "<center><img src=\"https://www.filfre.net/wp-content/uploads/2012/04/493394-breakout-commodore-pet-cbm-screenshot-since-i-couldn-t-get-300x220.png\" width=\"60%\"/></center>\n",
    "\n",
    "<center><a href=\"https://www.google.com/search?rlz=1C5CHFA_enUS836US836&biw=1920&bih=1001&tbm=isch&sa=1&ei=TXf4XPXJLsT_-gSuxKy4BA&q=atari+breakout&oq=a&gs_l=img.1.1.35i39l2j0i67l5j0j0i67j0.263688.265403..267036...1.0..0.132.259.0j2......0....1..gws-wiz-img.TNnYMFs-gE4\">Demo (Turn off Sound)</a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.filfre.net/2012/04/this-game-is-over/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "<center><img src=\"https://www.filfre.net/wp-content/uploads/2012/04/493394-breakout-commodore-pet-cbm-screenshot-since-i-couldn-t-get-300x220.png\" width=\"40%\"/></center>\n",
    "<br>\n",
    "<center>How many (states, action) pairs does ASCII Breakout have?</center>\n",
    "<center>What are actions?</center>\n",
    "<center>What are states?</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>What about the \"ball\" trajectory over time (Going Up? Going Down?) ?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is tabular Q-learning?</h2></center>\n",
    "\n",
    "<center><img src=\"images/q_table.png\" width=\"75%\"/></center>\n",
    "Q-values are stored in tabular form.\n",
    "\n",
    "The general tabular form is (States x Actions) with Utilities in the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the major limitations of tabular Q-learning?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Q-value table size very quickly scales out of memory (even across clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Also, continuous states can __not__ be stored in their raw form in any table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Most importantly - Q-values in tabular form are precise. However, we might never seen the same precise state with same actions again (sparsity). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Think of the states and branching factors of Go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>State Aggregation</h2></center>\n",
    "\n",
    "- State Aggregation is decreasing the state/action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Quantization, aka binning, is common technique.\n",
    "- Learn collections of states, instead of individual states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Typically reduce state space is a stop-gap measure. The Q-value table will just fill up with precise, aggregated values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Symmetry as Quantization</h2></center>\n",
    "<br><br>\n",
    "<center><img src=\"http://ggp.stanford.edu/notes/figures/symmetries.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Three symmetric positions in Tic-Tac-Toe. A rotation by 180¬∞ transforms the position shown on the left-hand side into the center board. Mirroring the latter along the first diagonal results in the position on the right-hand side.\n",
    "\n",
    "We only need to store 1 of these in the Q-value table.\n",
    "\n",
    "This is still precise Value Function.\n",
    "\n",
    "[Source](http://ggp.stanford.edu/notes/chapter_14.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Function Approximation (VFA)</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Use anything other than a lookup table.</center>\n",
    "<br>\n",
    "<center>You can be creative when defining a function</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Value Function Approximation (VFA)</h2></center>\n",
    " \n",
    " What about this a model of the Value Function?\n",
    " \n",
    " $$U(s) = w_1‚Ä¢f_1(s) + w_2‚Ä¢f_2(s) + ‚Ä¶ + w_n‚Ä¢f_n(s) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Linear regression (a weighted linear combination of features) to the rescue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Linear Regression Review</h2></center>\n",
    "\n",
    "<center><img src=\"images/li.png\" width=\"55%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Linear Regression Review</h2></center>\n",
    "\n",
    "Lossy data compression \n",
    "\n",
    "Input: Many raw points  \n",
    "Output: A few estimated coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"https://pediaa.com/wp-content/uploads/2018/08/Difference-Between-Lossy-and-Lossless-Compression-Comparison-Summary.jpg\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>What is the goal of Machine Learning (ML)?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<center>Learn a function from data that can <b>generalize</b> to predict new data.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Anyone who has taken an ML course with me knows that I consider generalization the core promise and problem of ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> The true power of Value Function Approximation is Generalization.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "I don't need to see every state to make a reasonable prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Function Approximation (VFA) Formalism</h2></center>\n",
    "\n",
    "$$\\hat q ( s , a , \\mathbf w ) ‚âà q_œÄ ( s , a )$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Approximate the states based a weight vector (either linear regression or Deep neural network)\n",
    "\n",
    "That weight vector and approximate state is much smaller than precise raw states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Update parameter $ \\mathbf w $ using MC or TD learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Function Approximation (VFA)</h2></center>\n",
    "\n",
    "<center><img src=\"images/blackbox.png\" width=\"25%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Value Function Approximation (VFA)</h2></center>\n",
    "\n",
    "<center><img src=\"images/whitebox.png\" width=\"25%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/actions_out.png\" width=\"25%\"/></center>\n",
    "\n",
    "Input: State  \n",
    "Output: An approximate estimate of the value for all possible actions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Questions about VFA?</h2></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.youtube.com/watch?v=UoPei5o4fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What are possible functions for VFA?\n",
    "\n",
    "Brainstorm a list with colleagues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>(An incomplete list of) Possible Functions for VFA</h2></center>\n",
    "\n",
    "- Linear Regression\n",
    "- Decision Tree, Random Forest, SVM, ... (Any supervised regression technique)\n",
    "- Fourier / wavelet bases (any non-learned linear or nonlinear function approximation method)\n",
    "- __Deep Learning__\n",
    "- A technique not yet invented!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"https://www.researchgate.net/profile/Vlatko_Cingoski/publication/224273877/figure/fig2/AS:339900940472342@1458050107468/Wavelets-and-function-approximation_W640.jpg\" width=\"35%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Survey of Nonlinear approximation methods](http://people.math.sc.edu/devore/publications/NLACTA.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2> Life is too short for Linear Regression as a VFA</h2></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Do __not__ define a model by hand. You are often __not__ a domain expert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - We do __not__ need to understand / interpret the model, just need to good predictions. Heck - We do __not__ understand / interpret what policy our agent is using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Linear regression needs to explicitly model non-linearities. Deep Learning does it automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reinforcement Learning has often big (nearly unlimited) data because it happens in a simulated world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our data is often non-stationary, non-iid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Neural networks can train end-to-end, thus better handle non-stationary, non-iid data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Check for understanding</h2></center>\n",
    " \n",
    " What are the biggest 4 limitations of storing values in a lookup table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Table size quickly grows.\n",
    "1. Can not handle continuous states.\n",
    "1. Sparsity - each state might only visited once or a hand-full of times. Most states are never visited.\n",
    "1. Brittle - specific to precisely observed state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Summary</h2></center>\n",
    "\n",
    "- Q-learning can be made more powerful through Value Function Approximation (VFA)\n",
    "- VFA learns a function to model state, action pairs.\n",
    "- VFA's #1 benefit is better ability to generalize to unseen states.\n",
    "- VFA is best done with Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
