{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Q-Learning\" data-toc-modified-id=\"Q-Learning-1\">Q-Learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#By-the-end-of-this-session,-you-should-be-able-to:\" data-toc-modified-id=\"By-the-end-of-this-session,-you-should-be-able-to:-2.0.1\">By the end of this session, you should be able to:</a></span></li></ul></li></ul></li><li><span><a href=\"#Student-Activity:-Review\" data-toc-modified-id=\"Student-Activity:-Review-3\">Student Activity: Review</a></span></li><li><span><a href=\"#What-is-&quot;Dots-and-Boxes&quot;?\" data-toc-modified-id=\"What-is-&quot;Dots-and-Boxes&quot;?-4\">What is \"Dots and Boxes\"?</a></span></li><li><span><a href=\"#Review\" data-toc-modified-id=\"Review-5\">Review</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-vs-TD(0)-Algorithm\" data-toc-modified-id=\"Monte-Carlo-(MC)-vs-TD(0)-Algorithm-6\">Monte Carlo (MC) vs TD(0) Algorithm</a></span></li><li><span><a href=\"#Q-Learning-Example\" data-toc-modified-id=\"Q-Learning-Example-7\">Q-Learning Example</a></span></li><li><span><a href=\"#Choose-your-own-adventure:-The-Knight-&amp;-The-Castle\" data-toc-modified-id=\"Choose-your-own-adventure:-The-Knight-&amp;-The-Castle-8\">Choose your own adventure: The Knight &amp; The Castle</a></span></li><li><span><a href=\"#The-Knight-&amp;-The-Castle\" data-toc-modified-id=\"The-Knight-&amp;-The-Castle-9\">The Knight &amp; The Castle</a></span></li><li><span><a href=\"#Q-Table-(aka,-Quality-of-State-Table)\" data-toc-modified-id=\"Q-Table-(aka,-Quality-of-State-Table)-10\">Q-Table (aka, Quality of State Table)</a></span></li><li><span><a href=\"#-RL-Loop\" data-toc-modified-id=\"-RL-Loop-11\"> RL Loop</a></span></li><li><span><a href=\"#-Q-learning\" data-toc-modified-id=\"-Q-learning-12\"> Q-learning</a></span></li><li><span><a href=\"#Q-value-\" data-toc-modified-id=\"Q-value--13\">Q-value </a></span></li><li><span><a href=\"#How-to-choose-in-$a$-in-$Q(s,-a)$-----------?\" data-toc-modified-id=\"How-to-choose-in-$a$-in-$Q(s,-a)$-----------?-14\">How to choose in $a$ in $Q(s, a)$           ?</a></span></li><li><span><a href=\"#How-are-Q's-learned,-aka-Q-learning?\" data-toc-modified-id=\"How-are-Q's-learned,-aka-Q-learning?-15\">How are Q's learned, aka Q-learning?</a></span></li><li><span><a href=\"#How-are-Q's-learned,-aka-Q-learning?\" data-toc-modified-id=\"How-are-Q's-learned,-aka-Q-learning?-16\">How are Q's learned, aka Q-learning?</a></span></li><li><span><a href=\"#Q-learning-Flow\" data-toc-modified-id=\"Q-learning-Flow-17\">Q-learning Flow</a></span></li><li><span><a href=\"#-Q-learning-Pseudocode\" data-toc-modified-id=\"-Q-learning-Pseudocode-18\"> Q-learning Pseudocode</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-19\">Check for understanding</a></span></li><li><span><a href=\"#Q-learning-Formalism\" data-toc-modified-id=\"Q-learning-Formalism-20\">Q-learning Formalism</a></span></li><li><span><a href=\"#$Œ±$:-Learning-Rate\" data-toc-modified-id=\"$Œ±$:-Learning-Rate-21\">$Œ±$: Learning Rate</a></span></li><li><span><a href=\"#$Œ±$:-Learning-Rate\" data-toc-modified-id=\"$Œ±$:-Learning-Rate-22\">$Œ±$: Learning Rate</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-23\">Check for understanding</a></span></li><li><span><a href=\"#Why-is-Q-learning-useful?\" data-toc-modified-id=\"Why-is-Q-learning-useful?-24\">Why is Q-learning useful?</a></span></li><li><span><a href=\"#Q-learning\" data-toc-modified-id=\"Q-learning-25\">Q-learning</a></span></li><li><span><a href=\"#Model-based-vs-Model-free-Algorithms\" data-toc-modified-id=\"Model-based-vs-Model-free-Algorithms-26\">Model-based vs Model-free Algorithms</a></span></li><li><span><a href=\"#5-elements-of-MDP\" data-toc-modified-id=\"5-elements-of-MDP-27\">5 elements of MDP</a></span></li><li><span><a href=\"#Model-Free-Algorithms\" data-toc-modified-id=\"Model-Free-Algorithms-28\">Model-Free Algorithms</a></span></li><li><span><a href=\"#Model-Free-Algorithms\" data-toc-modified-id=\"Model-Free-Algorithms-29\">Model-Free Algorithms</a></span></li><li><span><a href=\"#Game-of-Go\" data-toc-modified-id=\"Game-of-Go-30\">Game of Go</a></span></li><li><span><a href=\"#On-policy-vs-Off-policy\" data-toc-modified-id=\"On-policy-vs-Off-policy-31\">On-policy vs Off-policy</a></span></li><li><span><a href=\"#Challenge-Question\" data-toc-modified-id=\"Challenge-Question-32\">Challenge Question</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-33\">Check for understanding</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-34\">Summary</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-35\">Bonus Material</a></span></li><li><span><a href=\"#Alternative-Q-learning-Formulas\" data-toc-modified-id=\"Alternative-Q-learning-Formulas-36\">Alternative Q-learning Formulas</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-Learning</h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://i.vas3k.ru/7vu.jpg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "#### By the end of this session, you should be able to:\n",
    "\n",
    "- Write the Q-learning formula.\n",
    "- Define the elements of Q-learning in your own words.\n",
    "- Explain why Q-learning is so powerful and popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity: Review</h2></center>\n",
    "\n",
    "Define each of these concepts in your own words: \n",
    "\n",
    "- Policy œÄ\n",
    "- Value $V_{œÄ(s)}$\n",
    "- Monte Carlo (MC) Algorithms\n",
    "- Temporal difference (TD) Algorithms, specifically TD(0)\n",
    "\n",
    "Then apply those concepts to the _Dots and Boxes_ game.\n",
    "\n",
    "- What is a bad policy?\n",
    "- What is a good policy?\n",
    "- What is a high Value state, aka what is a good board setup to win?\n",
    "- How would a Monte Carlo Algorithm learn? How would a TD(0) Algorithm learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is \"Dots and Boxes\"?</h2></center> \n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Dots-and-boxes.svg/300px-Dots-and-boxes.svg.png\" width=\"45%\"/></center>\n",
    "\n",
    "<center><a href=\"https://www.coolmathgames.com/0-dots-and-boxes\">Demo!</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Review</h2></center>\n",
    "\n",
    "Policy (œÄ): \n",
    "\n",
    "- Maps states to actions\n",
    "- The strategy that the agent employs to determine next action based on the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Value is the expected, discounted, long-term return, as opposed to the short-term reward R. \n",
    "\n",
    "$V_{œÄ(s)}$ is  the expected, discounted, long-term return of the current state s following policy œÄ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) vs TD(0) Algorithm</h2></center>\n",
    "\n",
    "MC must wait for a complete game to update. A complete game is called an episode.\n",
    "\n",
    "TD(0) can update after each action (if there is a reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-Learning Example</h2></center>\n",
    "\n",
    "<center><img src=\"http://blog-assets.bigfishgames.com/uploads/2012/08/Choose-your-own-adventure.jpg\" width=\"100%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Choose your own adventure: The Knight & The Castle</h2></center>\n",
    "\n",
    "You are young knight ‚öî. You have your well-being and looking for treasure. You find castle üè∞ You enter and find it an abandoned  ‚Ä¶\n",
    "\n",
    "It is spooky üëª but intriguing üòØ. The Entrance/Exit \"E\" room gives you sense of wonder. Your well-being is +5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You see two doors. One label \"A\" and the other labeled \"D\".\n",
    "\n",
    "What do you do?\n",
    "\n",
    "- Go through door \"A\"\n",
    "- Go through door \"D\"\n",
    "- Stay in Entrance/Exit \"E\" room\n",
    "- Or leave the castle\n",
    "\n",
    "(You might want to take notes of where (State) and what happens (Reward).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"images/Modeling-Environment_clip_image002.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "<center><img src=\"images/adventure.png\" width=\"75%\"/></center>\n",
    "\n",
    "https://www.draw.io/#G17aMDvycTke0xjgYj5EV7SMpbCmNGYEoo\n",
    "\n",
    "Rewards:\n",
    "\n",
    "- E - Entrance +5 well-being (awe)\n",
    "- A - Apartment -10 well-being (musty and moldy)\n",
    "- B - Buttery +10 well-being (butter is delicious)\n",
    "- D - Keep 0 well-being\n",
    "- C - Dragon's Den [-15, -30, -60, -120, ‚Ä¶] well-being  and will take all your treasure.\n",
    "- F - Treasure room -20 well-being (damp) but [+100, +100, +100, 0, 0, 0, ‚Ä¶] treasure\n",
    "\n",
    "Rules:\n",
    "\n",
    "- It is dark and you get turned around don't always do want you intend (stochastic).\n",
    "- Can stay in any room as long you want.\n",
    "- A room only gives reward 3 times. After the room gives `0` (expect Dragon's Den which just worse and worse)\n",
    "    - Can collect the treasure 3 times, then there is no more treasure to collect.\n",
    "- The dragon gets angrier every time you enter its state and burns you more each time.\n",
    "- The dragoon will take all your treasure.\n",
    "- You should try to exit through entrance.\n",
    "\n",
    " [Source](http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>The Knight & The Castle</h2></center>\n",
    "\n",
    "Is this a MDP?\n",
    "\n",
    "Can this be solved with Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ Sequential? Be explicit on the time steps (e.g., discrete, continuous, finite, infinite)\n",
    "+ Environment\n",
    "+ Agent - Who/what takes an action and gets a reward?\n",
    "+ State - What are all possible states the agent can be in?\n",
    "+ Reward - What are the all the rewards in each state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Technically - The Knight & The Castle is not Markov because rooms have state-based history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 110%;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 110%;\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-Table (aka, Quality of State Table)</h2></center>\n",
    "<br>\n",
    "\n",
    "| State | Value  |  \n",
    "|:-------:|:------|\n",
    "| E (Entrance) | +12.5 (a sense of awe) |\n",
    "| A  (Apartment)  | -13.6 (moldy and musty ) |  \n",
    "| B  (Buttery)  | +11.8 (Butter is delicious) |  \n",
    "| C  (Dragoon's den)  | -59.8 (üêâ üî•) |  \n",
    "| D  (Keep)  | 0 (Nothing) |  \n",
    "| F  (Treasure)  | +252.3 (üí∞üíéüëë)|  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "The values are only approximate numbers!\n",
    "\n",
    "\n",
    "Brian needs to actual calculate the values based on the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> RL Loop</h2></center>\n",
    " \n",
    "<center><img src=\"images/loop.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2> Q-learning</h2></center>\n",
    " \n",
    " <center><img src=\"images/loop.png\" width=\"20%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Run Policy - Take a single step in Environment. Either random or greedy (the best policy we have so far)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evaluate Policy - After taking action following current policy, store results (s, a, s‚Ä≤, r). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Improve Policy - What is the next best action to maximize Utility in the current state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source:\n",
    "- Spinning Up in Deep RL Workshop from OpenAI from 1:48 to 2:25\n",
    "    - [video](https://youtu.be/fdY7dt3ijgY?t=6482)\n",
    "    - [slides start at #48](https://github.com/openai/spinningup-workshop/blob/master/rl_intro/rl_intro.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-value </h2></center>\n",
    "\n",
    "Q-value $Q(s, a)$ is the expected discounted reward if we perform action $a$ in state $s$, then follow optimal policy from then on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Q-value is similar to Value, except that Q-value takes the current action $a$ as additional parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$Q_œÄ(s, a)$ refers to the long-term return of the current state s, taking action $a$ under policy $œÄ$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How to choose in $a$ in $Q(s, a)$           ?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Œµ-Greedy\n",
    "\n",
    "- Ensuring continual exploration\n",
    "- With probability 1 - Œµ  choose the greedy action \n",
    "- With probability Œµ  choose an action at random\n",
    "\n",
    "<center><img src=\"images/e_greedy.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How are Q's learned, aka Q-learning?</h2></center>\n",
    "\n",
    "$Q$ learns to corresponds to taking the action $a$ and then continuing under a policy $œÄ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This function is unknown.\n",
    "\n",
    "We'll learn about its Value through experience, based on $(s, a, s')$ triples. \n",
    "\n",
    "\"I was in state $s$, and I tried doing $a$ and $s'$ happened.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How are Q's learned, aka Q-learning?</h2></center>\n",
    "\n",
    "Before we explore the environment, Q gives the same (arbitrary) fixed value. \n",
    "\n",
    "But then, as we explore the environment more, Q gives us a better and better approximation of the value of an action a at a state s. \n",
    "\n",
    "We update our function Q as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " We'll store the results in a Q-table which is state and reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Flow</h2></center>\n",
    "\n",
    "<center><img src=\"images/q_diagram.jpg\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Q-learning Pseudocode</h2></center>\n",
    "\n",
    "Initialize the Values table $Q(s, a)$ \n",
    "\n",
    "Repeat until a terminal state is reached:\n",
    "\n",
    "1. Observe the current state $s$.\n",
    "1. Choose an action $a$ based on one of the action selection policies (i.e., epsilon greedy)\n",
    "1. Take the action, and observe the reward $r$ as well as the new state $s$.\n",
    "1. Update the Value for the state using the observed reward and the maximum reward possible for the next state. \n",
    "1. Set the current state to the new state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "How could we initialize the Values table  ùëÑ(ùë†,ùëé)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Random \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Deterministic Neutral - A value of 0. Then updating must be forced, for example through game play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Deterministic Optimistic - Something good could happen. Encourage exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Something else "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Formalism</h2></center>\n",
    "<br>  \n",
    "\n",
    "<center><img src=\"images/q_formula_annotated.svg\" width=\"100%\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated value is a combination of the previous value plus the weighted amount of reward and discounted future rewards if we get the best possible value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Without annotations (useful for studying for exam):\n",
    "\n",
    "<center><img src=\"images/q.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>$Œ±$: Learning Rate</h2></center>\n",
    "\n",
    "The learning rate determines to what extent the newly acquired information will override the old information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When Œ±=0, the agent not learn anything. Exclusively exploit prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When Œ±=1, the agent consider only the most recent information. Ignoring prior knowledge to explore possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>$Œ±$: Learning Rate</h2></center>\n",
    "\n",
    "In practice, the learning rate is often a low constant.\n",
    "\n",
    "$$Œ± = 0.1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "In fully deterministic environments, what should learning rate $Œ±$ be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$Œ± = 1$ is optimal in fully deterministic environments. \n",
    "\n",
    "The most recent information is all that is needed because the future will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why is Q-learning useful?</h2></center>\n",
    "\n",
    "Finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and all successive steps, starting from the current state without a model of the Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$œÄ^*(s) = argmax Q(s,a)$$\n",
    "\n",
    "Just chose the action that maximizes the Q value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>A <b>model-free, off-policy</b> technique to learn an optimal Q(s, a) for the Agent.</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Model-based vs Model-free Algorithms</h2></center>\n",
    " \n",
    "Model-based algorithms assume complete knowledge of 5 elements of MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Quick - write down the 5 elements of MDP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>5 elements of MDP</h2></center>\n",
    "\n",
    "<center>(S,A,P,R,Œ≥)</center>\n",
    "\n",
    "1. State\n",
    "2. Action\n",
    "3. Transition Probabilities  \n",
    "4. Reward\n",
    "5. Discounting rate of future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Model-Free Algorithms</h2></center>\n",
    "\n",
    "MDP model is unknown but can be sampled.\n",
    "\n",
    "As we experience the MDP, we learn the States, Actions, Transition Probabilities, Reward, and Discounting rate of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Model-Free Algorithms</h2></center>\n",
    "\n",
    "Model-Free Algorithms are also used when MDP model is known, but it is computationally infeasible to use directly (except through sampling).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is an example of computationally infeasible, but completely known MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Game of Go</h2></center>\n",
    "\n",
    "<center><img src=\"images/go.jpeg\" width=\"50%\"/></center>\n",
    "\n",
    "Since each location on the board can be either empty, black, or white, there are a total of $3^{n^2}$ possible board positions on a square board with length n (However, only part of them are legal).\n",
    "\n",
    "[Let‚Äôs look at a table](https://en.wikipedia.org/wiki/Go_and_mathematics#Legal_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Learn more about the game complexity of Go](https://en.wikipedia.org/wiki/Go_and_mathematics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>On-policy vs Off-policy</h2></center>\n",
    " \n",
    " __On-policy__: \n",
    " \n",
    "- Learns optimal policy only based on an agent following the current policy.\n",
    " \n",
    "- Updates Q-values using the Q-value of the next state ùë†‚Ä≤ and the current policy's action ùëé‚Ä≥. Estimates the return for state-action pairs assuming the current policy continues to be followed.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " __Off-policy__: \n",
    "\n",
    "- Learns optimal policy independently of agent's actions.\n",
    " \n",
    "- Updates Q-values using the Q-value of the next state ùë†‚Ä≤ and the greedy action ùëé‚Ä≤. Estimates the return for state-action pairs assuming a greedy policy were followed despite the fact that it's not following a greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://stats.stackexchange.com/questions/184657/difference-between-off-policy-and-on-policy-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Challenge Question</h2></center>\n",
    "\n",
    "If your state space very large and you still want to apply Q-learning, what should you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Compression through Value Function Approximation. \n",
    "\n",
    "One example is feature engineering, learn a useful compression representation of state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What technique is very good at feature engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/deep_learning.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image Source](https://becominghuman.ai/deep-learning-made-easy-with-deep-cognition-403fbe445351)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Q-learning is the most useful RL algorithm we have seen thus far.\n",
    "- Because Q-learning is a __model-free__, __off-policy__ technique.\n",
    "- Q-learning learns the best policy for given state by empirically taking actions and sampling rewards.\n",
    "- Q-learning formalism:\n",
    "<center><img src=\"images/q.png\" width=\"75%\"/></center> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Alternative Q-learning Formulas</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/63b502aafbe6ea1585231222ea3783f40f0808a9\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/another_version_q.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
