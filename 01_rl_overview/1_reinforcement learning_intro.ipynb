{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "# <h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Reinforcement-Learning\" data-toc-modified-id=\"Introduction-to-Reinforcement-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to Reinforcement Learning</a></span></li><li><span><a href=\"#By-The-End-Of-This-Session-You-Should-Be-Able-To:\" data-toc-modified-id=\"By-The-End-Of-This-Session-You-Should-Be-Able-To:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>By The End Of This Session You Should Be Able To:</a></span></li><li><span><a href=\"#Reinforcement-Learning-is-the-new-awesomeness\" data-toc-modified-id=\"Reinforcement-Learning-is-the-new-awesomeness-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Reinforcement Learning is the new awesomeness</a></span></li><li><span><a href=\"#Build-Up-Diagram\" data-toc-modified-id=\"Build-Up-Diagram-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build Up Diagram</a></span></li><li><span><a href=\"#What-is-Reinforcement-Learning?\" data-toc-modified-id=\"What-is-Reinforcement-Learning?-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>What is Reinforcement Learning?</a></span></li><li><span><a href=\"#Reinforcement-Learning:-Another-definition\" data-toc-modified-id=\"Reinforcement-Learning:-Another-definition-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Reinforcement Learning: Another definition</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Check for understanding</a></span></li><li><span><a href=\"#Why-does-RL-go-well-with-video-games?\" data-toc-modified-id=\"Why-does-RL-go-well-with-video-games?-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Why does RL go well with video games?</a></span></li><li><span><a href=\"#What-domains-are-well-suited-for-Reinforcement-Learning?-\" data-toc-modified-id=\"What-domains-are-well-suited-for-Reinforcement-Learning?--9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>What domains are well-suited for Reinforcement Learning? </a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Student Activity</a></span></li><li><span><a href=\"#Comparison-Shopping:-RL,-UL,-&amp;-RL-\" data-toc-modified-id=\"Comparison-Shopping:-RL,-UL,-&amp;-RL--11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Comparison Shopping: RL, UL, &amp; RL </a></span></li><li><span><a href=\"#What-is-the-differences-between-Supervised,-Unsupervised,-and-Reinforcement-Learning?\" data-toc-modified-id=\"What-is-the-differences-between-Supervised,-Unsupervised,-and-Reinforcement-Learning?-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>What is the differences between Supervised, Unsupervised, and Reinforcement Learning?</a></span></li><li><span><a href=\"#Student-Activity:-Fill-in-the-following-table\" data-toc-modified-id=\"Student-Activity:-Fill-in-the-following-table-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Student Activity: Fill in the following table</a></span></li><li><span><a href=\"#What-is-an-example-task-for-each-type?\" data-toc-modified-id=\"What-is-an-example-task-for-each-type?-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>What is an example task for each type?</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Check for understanding</a></span></li><li><span><a href=\"#Temporal-Credit-Assignment-Problem\" data-toc-modified-id=\"Temporal-Credit-Assignment-Problem-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Temporal Credit Assignment Problem</a></span></li><li><span><a href=\"#What-are-Reinforcement-Learning-limitations-(aka,-when-would-an-agent-not-learn-the-optimal-action)?\" data-toc-modified-id=\"What-are-Reinforcement-Learning-limitations-(aka,-when-would-an-agent-not-learn-the-optimal-action)?-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>What are Reinforcement Learning limitations (aka, when would an agent not learn the optimal action)?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Violate-one-of-the-constraints\" data-toc-modified-id=\"Violate-one-of-the-constraints-17.1\"><span class=\"toc-item-num\">17.1&nbsp;&nbsp;</span>Violate one of the constraints</a></span></li><li><span><a href=\"#Violate-one-of-the-constraints\" data-toc-modified-id=\"Violate-one-of-the-constraints-17.2\"><span class=\"toc-item-num\">17.2&nbsp;&nbsp;</span>Violate one of the constraints</a></span></li><li><span><a href=\"#Violate-one-of-the-constraints\" data-toc-modified-id=\"Violate-one-of-the-constraints-17.3\"><span class=\"toc-item-num\">17.3&nbsp;&nbsp;</span>Violate one of the constraints</a></span></li></ul></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>Check for understanding</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Further-Study\" data-toc-modified-id=\"Further-Study-20\"><span class=\"toc-item-num\">20&nbsp;&nbsp;</span>Further Study</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Introduction to Reinforcement Learning</h2></center>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://www1.vizury.com/blog/wp-content/uploads/2017/06/RL-article-image.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "#### By the end of this session, you should be able to:\n",
    "\n",
    "- Recognize examples of Reinforcement Learning. \n",
    "- Define elements of Reinforcement Learning problem formation:\n",
    "    - Agent\n",
    "    - Environment\n",
    "    - State\n",
    "    - Action\n",
    "    - Reward\n",
    "- Compare Reinforcement Learning to Supervised Learning and Unsupervised Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning (RL) is the new awesomeness</h2></center>\n",
    "\n",
    "<center><img src=\"https://ichef.bbci.co.uk/news/660/cpsprodpb/11B23/production/_88738427_pic1go.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>RL is very good at video games</h2></center>\n",
    "\n",
    "<center><img src=\"https://images-na.ssl-images-amazon.com/images/I/71ipTP6sGFL._SY679_.jpg\" width=\"35%\"/></center>\n",
    "\n",
    "<center><a href=\"https://www.google.com/search?q=pacman&rlz=1C5CHFA_enUS836US836&oq=pacman&aqs=chrome..69i57.879j0j9&sourceid=chrome&ie=UTF-8\">PAC-MAN demo</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning Setup</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Build Up Diagram of RL\n",
    "------\n",
    "\n",
    "1. Environment - Current Board\n",
    "1. Agent - Pacman or Ms. Pacman (my favorite)\n",
    "1. Observe State \n",
    "    - We can ask ourselves what does the agent (Pac-Man) need to know in order to make a good move? It needs to know its own position, positions of all the ghosts and position of all the food. Right? So, a good representation of the state might be a tuple containing values of\n",
    "    - Where am I?\n",
    "    - Where are the walls and paths?\n",
    "    - Where are the ghosts?\n",
    "    - Where are the pellets?\n",
    "    - What could be an efficient way to represent a state in this game? \n",
    "1. Action \n",
    "    - Our agent can move only in four directions in this 2D grid so our actions are {Up,Down,Left,Right}\n",
    "1. Maximize Reward\n",
    "    - Our end goal is to obtain maximum possible score in the game. So, after performing an action, the increase in score can be a good reward signal for our agent. Or, if your intention is to survive in the game as long as possible, time spent in the game can be a good reward signal for the agent.\n",
    "    - Positive - eat stuff, points go up\n",
    "    - Negative - die\n",
    "    - Long term\n",
    "        - short term - eat closest pellet near ghost\n",
    "        - long term - eat farther away pellets, live longer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://medium.com/code-heroku/introduction-to-reinforcement-learning-67826ec177ea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>What is Reinforcement Learning?</h2></center>\n",
    "\n",
    "<center><img src=\"https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/04/aeloop-300x183.png\" width=\"75%\"/></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"images/mdp.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://www.ausy.tu-darmstadt.de/uploads/Research/Research/ReinforcementLearning.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"images/figtmp7.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning: Another Definition</h2></center>\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/trial.jpg\" width=\"35%\"/></center>\n",
    "\n",
    "<center>Learn to make a sequence of decisions, by trial & error, in order to achieve a goal.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Why does RL go well with video games?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Well-defined environment\n",
    "- Simple actions\n",
    "- Clear, delayed rewards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What domains are well-suited for Reinforcement Learning? \n",
    "-------\n",
    "\n",
    "For example, video games. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Board games (Go, Backgammon, Chess, …)\n",
    "- Robotics (Self-driving cars, warehouse robots, …)\n",
    "- Code (SQL query optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](http://karpathy.github.io/2016/05/31/rl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning for SQL Query Optimization</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```sql\n",
    "SELECT Customers.CustomerID, Customers.Name, Sales.LastSaleDate\n",
    "FROM Customers, Sales\n",
    "WHERE Customers.CustomerID = Sales.CustomerID\n",
    "```\n",
    "\n",
    "Is this an optimal query?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This type of join creates a Cartesian Join, also called a Cartesian Product or CROSS JOIN. In a Cartesian Join, all possible combinations of the variables are created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```sql\n",
    "SELECT Customers.CustomerID, Customers.Name, Sales.LastSaleDate\n",
    "FROM Customers\n",
    "   INNER JOIN Sales\n",
    "   ON Customers.CustomerID = Sales.CustomerID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To prevent creating a Cartesian Join, INNER JOIN should be used instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>SQL Query Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"https://docs.oracle.com/database/121/TGSQL/img/GUID-22630970-B584-41C9-B104-200CEA2F4707-default.gif\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>SQL Query Optimization as RL problem</h2></center>\n",
    "\n",
    "- How is it sequential? (If not sequential, pick new task)\n",
    "- What is the Environment?\n",
    "- What is the Agent?\n",
    "- What are example States?\n",
    "- What are example Rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>SQL Query Optimization as RL problem</h2></center>\n",
    "\n",
    "- How is it sequential? (If not sequential, pick new task)\n",
    "    - Yes. The query execute items in order and does it over-and-over\n",
    "- What is the Environment?\n",
    "    - The entire database system\n",
    "- What is the Agent?\n",
    "    - The optimizer \n",
    "- What are example States?\n",
    "    - The current query plan\n",
    "- What are example Actions?\n",
    "    - Represents an individual change to the query plan\n",
    "- What are example Rewards?\n",
    "    - Reduce query steps\n",
    "    - Less data being moved from disk\n",
    "    - Lower execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning a join order enumerator</h2></center>\n",
    "\n",
    "<center><img src=\"https://adriancolyer.files.wordpress.com/2019/01/DL-Optimizer-Fig-1.jpeg?w=480\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Learning a join order enumerator</h2></center>\n",
    "\n",
    "<center><img src=\"https://adriancolyer.files.wordpress.com/2019/01/DL-Optimizer-Fig-2.jpeg?w=640\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Results</h2></center>\n",
    "<center><img src=\"https://adriancolyer.files.wordpress.com/2019/01/DL-Optimizer-Fig-3a.jpeg?w=420\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Please read the following:\n",
    "\n",
    "- [Blogpost](https://blog.acolyer.org/2019/01/18/towards-a-hands-free-query-optimizer-through-deep-learning/)\n",
    "- [Technical Paper](http://cidrdb.org/cidr2019/papers/p96-marcus-cidr19.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "In small groups, pick a specific task for RL (not yet talked about). \n",
    "\n",
    "Define the following elements:\n",
    "\n",
    "- How is it sequential? (If not sequential, pick new task.)\n",
    "- What is the Environment?\n",
    "- What is the Agent?\n",
    "- What are example States?\n",
    "- What are example Actions?\n",
    "- What are example Rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>The promise and peril of RL: State</h2></center>\n",
    "\n",
    "<center><img src=\"images/mdp.png\" width=\"75%\"/></center>\n",
    "\n",
    "<center>RL can learn state.</center>\n",
    "\n",
    "<center>State makes programming hard.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement learning (RL)</h2></center>\n",
    "<br>\n",
    "<center>Algorithms that learn how agents ought to take sequences of actions in an environment in order to maximize cumulative rewards.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/types.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are example tasks and algorithms for each type of Machine Learning?</h2></center>\n",
    "\n",
    "- Supervised Learning (SL) \n",
    "- Unsupervised Learning (UL) \n",
    "- Reinforcement (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are example tasks for each type of Machine Learning?</h2></center>\n",
    "\n",
    "- Supervised Learning (SL) - Classification & Regression\n",
    "- Unsupervised Learning (UL)  - Clustering, dimensionality reduction, representation learning, density estimation, …\n",
    "- RL - Wait for the rest of the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/mind_map.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the differences between Supervised, Unsupervised, and Reinforcement Learning?</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 100%;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity: Fill in the following table</h2></center>\n",
    "\n",
    "|  Compare:      | Supervised  | Unsupervised  | Reinforcement Learning | \n",
    "| :----: | :-----------: | :-------------: | :------: |  \n",
    "| Goals   |              |                |         |\n",
    "| Labels  |              |                |  |\n",
    "| Batch or Stream |      |                |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "|  Compare:      | Supervised  | Unsupervised  | Reinforcement Learning |  \n",
    "|:-------:|:-----------:|:-------------:|:------:|  \n",
    "| Goals   |    Learn a function that maps input to output         |    Learn the latent structure of the data           |   Learn to maximize rewards     |\n",
    "| Labels  |    Every data point has label         |   None            | Sparse & Time delayed |\n",
    "| Batch or Stream | Batch    |    Batch           |Stream |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://blogs.sas.com/content/subconsciousmusings/2017/09/25/machine-learning-concepts-styles-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/another_table.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Comparison Shopping: RL, UL, & RL </h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*KDvA9Fq3lm-eQOyGlcKAKg.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://medium.com/intuitionmachine/predictive-learning-is-the-key-to-deep-learning-acceleration-93e063195fd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Supervised learning and Reinforcement Learning both need labeled data. What is the difference between SL and RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Supervised learning __requires__ every data point have a label for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"images/rewards.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "<center>Reinforcement Learning can handle <b>sparse</b> and <b>time-delayed</b> labels, aka rewards. </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Supervised learning is “teach by example”.</h2></center>\n",
    "\n",
    "<center><h2>Reinforcement learning is “teach by experience”.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://louiskirsch.com/assets/posts/map-reinforcement-learning/overview.svg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](http://louiskirsch.com/maps/reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are Reinforcement Learning limitations (aka, when would an agent not learn)?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Violate one of the constraints\n",
    "\n",
    "- Agent\n",
    "    - Can not take action that would lead to a reward\n",
    "    - Can not process reward\n",
    "- Rewards\n",
    "    - Not enough rewards (very low rate or \"binary\")\n",
    "    - Rewards are not based on agent performance (random or biased)\n",
    "    - Rewards are too delayed or hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What is technical name for \"the best rewards\"?\n",
    "\n",
    "What if they are hard to get to and you have settle for lower value rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Global minimum is the best rewards\n",
    "\n",
    "2. Local minimum are easier to find but lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Reinforcement Learning is a sub-field of Machine Learning that is becoming increasingly popular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reinforcement Learning: An agent learns to maximize cumulative rewards in an environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- RL can learn sparse, time-delayed rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Every Reinforcement Learning problems has the following elements:\n",
    "    - Sequential\n",
    "    - Environment\n",
    "    - Agent\n",
    "    - State\n",
    "    - Action\n",
    "    - Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One of the greatest challenges in applying RL to real-world problems is deciding how to represent those elements within existing RL frameworks to much computation feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Study\n",
    "------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/gartner_hype_cycle.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/hype_cycle.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Temporal Credit Assignment Problem\n",
    "-----\n",
    "\n",
    "Which of the preceding actions was responsible for getting the reward and to what extent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The biggest problem facing a reinforcement-learning agent.\n",
    "\n",
    "One strategy is to wait until the \"end\" and reward the actions taken if the result was good and punish them if the result was bad. \n",
    "\n",
    "In ongoing tasks, it is difficult to know what the \"end\" is, and this might require a great deal of memory. \n",
    "\n",
    "Instead, we will use insights from value iteration to adjust the estimated value of a state based on the immediate reward and the estimated value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why aren't we using X framework?</h2></center>\n",
    "\n",
    "There is a tension between building vs using using frameworks.\n",
    " \n",
    "This class is building.\n",
    "\n",
    "Why aren't we using Ray from UC Berkeley’s RISElab?\n",
    "\n",
    "1. See above\n",
    "1. Too new\n",
    "2. Designed for multi-agent reinforcement learning wich is out-of-scope\n",
    "\n",
    "You can use it on your Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
